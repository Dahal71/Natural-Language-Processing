{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26dc77ab",
   "metadata": {},
   "source": [
    "# Collecting the Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1817dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"StopWords_Auditor.txt\") as my_text:\n",
    "    a = my_text.read().lower().split()\n",
    "    uni_a = set(a) # getting unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156154e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coopers',\n",
       " 'deloitte',\n",
       " 'ernst',\n",
       " 'kpmg',\n",
       " 'pricewaterhouse',\n",
       " 'pricewaterhousecoopers',\n",
       " 'touche',\n",
       " 'young'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "728ebdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"StopWords_GenericLong.txt\") as my_txt:\n",
    "    b = my_txt.read().lower().split()\n",
    "    uni_b = set(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b291780",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"StopWords_DatesandNumbers.txt\") as my_text:\n",
    "    c = my_text.read().lower().split()\n",
    "    uni_c = set(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e52df9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"StopWords_Generic.txt\") as my_txt:\n",
    "    d = my_txt.read().lower().split()\n",
    "    uni_d = set(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac46b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"StopWords_Geographic.txt\") as my_txt:\n",
    "    e = my_txt.read().lower().split()\n",
    "    uni_e = set(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a12a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"StopWords_Names.txt\") as my_txt:\n",
    "    f = my_txt.read().lower().split()\n",
    "    uni_f = set(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a8bd639",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"StopWords_Currencies.txt\") as my_text:\n",
    "    g = my_text.read().lower().split()\n",
    "    uni_g = set(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e13fe9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_uni_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5356b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_uni_words.update(uni_a) # updating all the unique words\n",
    "all_uni_words.update(uni_b)\n",
    "all_uni_words.update(uni_c)\n",
    "all_uni_words.update(uni_d)\n",
    "all_uni_words.update(uni_e)\n",
    "all_uni_words.update(uni_f)\n",
    "all_uni_words.update(uni_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77cf82a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12840"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_uni_words) # this consists all the words across all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "207e9633",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab = dict()\n",
    "i = 0\n",
    "\n",
    "for word in all_uni_words:\n",
    "    full_vocab[word] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07b44223",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stop_words = [\"\"]*len(full_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aa0a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the dictionary to list\n",
    "for word in full_vocab:\n",
    "    word_index = full_vocab[word]\n",
    "    all_stop_words[word_index] += word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "002912f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12840"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04de4f",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a522af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce38f4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\user\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from textblob) (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b79bab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b3c47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50e8ea3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cbc87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f7ccb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baa3f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive_Score = []\n",
    "Negative_score = []\n",
    "Polarity_Score = []\n",
    "Subjectivity_Score = []\n",
    "URL = []\n",
    "Average_sentence_length = []\n",
    "Word_count = []\n",
    "Complex_words_count = []\n",
    "Percentage_of_complex_words = []\n",
    "Fog_Index = []\n",
    "Personal_pronoun_counts = []\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# adding our customized stop words\n",
    "custom_stop_words = all_stop_words\n",
    "stop_words.update(custom_stop_words)\n",
    "\n",
    "def analyze_sentiment(tweet):\n",
    "    analysis = TextBlob(tweet) #creating a TextBlob object from the tweet parameter\n",
    "    #extract the polarity and subjectivity scores from the TextBlob object using its sentiment property\n",
    "    polarity = analysis.sentiment.polarity \n",
    "    subjectivity = analysis.sentiment.subjectivity\n",
    "    return polarity, subjectivity\n",
    "\n",
    "df = pd.read_excel('C:\\\\Users\\\\User\\\\Downloads\\\\Input.xlsx', usecols=[1])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url = row[0]\n",
    "    response = requests.get(url)\n",
    "\n",
    "    soup = bs4.BeautifulSoup(response.content, \"lxml\")\n",
    "    text = soup.getText(strip=True)\n",
    "    \n",
    "    #cleaning content by importing the \"regular expression\" library\"\n",
    "    #import re # regular expression\n",
    "    text = re.sub(r'\\[\\d+\\]',\"\",text)\n",
    "    text = re.sub(r'\\[\\w+\\]',\"\",text)\n",
    "    text = re.sub('[0-9]+',\"\", text)\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentiment_scores = [analyze_sentiment(sentence) for sentence in sentences if sentence not in stop_words] # stop_words\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(text) # Tokenize the text into sentences\n",
    "\n",
    "    # Get the total number of words in all the sentences\n",
    "    total_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences)\n",
    "    \n",
    "    total_sentences = len(sentences) #the total number of sentences\n",
    "\n",
    "    # Calculating the average sentence length\n",
    "    avg_sentence_length = total_words / total_sentences\n",
    "\n",
    "    # Get the total number of complex words\n",
    "    complex_words = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            if len(word) > 2 and nltk.pos_tag([word])[0][1] in ['JJ', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', \n",
    "                                                                'RB', 'RBR', 'RBS']:\n",
    "                complex_words.append(word)\n",
    "    total_complex_words = len(complex_words)\n",
    "\n",
    "    # Calculating the percentage of complex words\n",
    "    percent_complex_words = (total_complex_words / total_words) * 100\n",
    "\n",
    "    # Get the total number of unique words\n",
    "    unique_words = set(nltk.word_tokenize(text))\n",
    "\n",
    "    # Get the total number of words\n",
    "    total_words = len(nltk.word_tokenize(text))\n",
    "    \n",
    "    # Define the list of personal pronouns to search for\n",
    "    personal_pronouns = [\"I\", \"we\", \"my\", \"us\", \"ours\"]\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Count the number of occurrences of each personal pronoun\n",
    "    personal_pronoun_counts = [words.count(pronoun) for pronoun in personal_pronouns]\n",
    "    personal_pronoun_counts = sum(personal_pronoun_counts) # summing the numbers of the list\n",
    "\n",
    "    # Count the number of positive and negative polarity scores\n",
    "    polarity_scores = [score[0] for score in sentiment_scores]\n",
    "    positive_scores = sum(1 for score in polarity_scores if score > 0)\n",
    "    negative_scores = sum(1 for score in polarity_scores if score < 0)\n",
    "\n",
    "    # Calculate the average polarity and subjectivity scores\n",
    "    avg_polarity = sum(polarity_scores) / len(polarity_scores)\n",
    "    avg_subjectivity = sum([score[1] for score in sentiment_scores]) / len(sentiment_scores)\n",
    "\n",
    "    Positive_Score.append(positive_scores)\n",
    "    Negative_score.append(negative_scores)\n",
    "    Polarity_Score.append(avg_polarity)\n",
    "    Subjectivity_Score.append(avg_subjectivity)\n",
    "    URL.append(url)\n",
    "    Average_sentence_length.append(avg_sentence_length)\n",
    "    Percentage_of_complex_words.append(percent_complex_words)\n",
    "    Fog_Index.append(0.4 * (avg_sentence_length + percent_complex_words))\n",
    "    Word_count.append(total_words)\n",
    "    Complex_words_count.append(total_complex_words)\n",
    "    Personal_pronoun_counts += [personal_pronoun_counts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f34e6e",
   "metadata": {},
   "source": [
    "# Calculating Average Number of words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46ef63b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Average_Number_of_words_per_sentence = []\n",
    "\n",
    "def analyze_sentiment(tweet):\n",
    "    analysis = TextBlob(tweet) #creating a TextBlob object from the tweet parameter\n",
    "    #extract the polarity and subjectivity scores from the TextBlob object using its sentiment property\n",
    "    polarity = analysis.sentiment.polarity \n",
    "    subjectivity = analysis.sentiment.subjectivity\n",
    "    return polarity, subjectivity\n",
    "\n",
    "df = pd.read_excel('C:\\\\Users\\\\User\\\\Downloads\\\\Input.xlsx', usecols=[1])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url = row[0]\n",
    "    response = requests.get(url)\n",
    "\n",
    "    soup = bs4.BeautifulSoup(response.content, \"lxml\")\n",
    "    text = soup.getText(strip=True)\n",
    "\n",
    "    text = re.sub(r'\\[\\d+\\]',\"\",text)\n",
    "    text = re.sub(r'\\[\\w+\\]',\"\",text)\n",
    "    text = re.sub('[0-9]+',\"\", text)\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    sentiment_scores = [analyze_sentiment(sentence) for sentence in sentences if sentence not in stop_words] # stop_words\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(text) # Tokenize the text into sentences\n",
    "\n",
    "    # Getting the total number of words in all the sentences\n",
    "    total_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences)\n",
    "    \n",
    "    total_sentences = len(sentences) #the total number of sentences\n",
    "    \n",
    "    Average_Number_of_words_per_sentence += [total_words / total_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90008b7d",
   "metadata": {},
   "source": [
    "# Calculating Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e67988fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_sentiment(tweet):\n",
    "    analysis = TextBlob(tweet)\n",
    "    polarity = analysis.sentiment.polarity \n",
    "    subjectivity = analysis.sentiment.subjectivity\n",
    "    return polarity, subjectivity\n",
    "\n",
    "def calculate_avg_word_length(text):\n",
    "    words = text.split()\n",
    "    total_length = sum(len(word) for word in words)\n",
    "    return total_length / len(words)\n",
    "\n",
    "df = pd.read_excel('C:\\\\Users\\\\User\\\\Downloads\\\\Input.xlsx', usecols=[1])\n",
    "\n",
    "Average_Word_Length = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url = row[0]\n",
    "    response = requests.get(url)\n",
    "\n",
    "    soup = bs4.BeautifulSoup(response.content, \"lxml\")\n",
    "    text = soup.getText(strip=True)\n",
    "\n",
    "    text = re.sub(r'\\[\\d+\\]',\"\",text)\n",
    "    text = re.sub(r'\\[\\w+\\]',\"\",text)\n",
    "    text = re.sub('[0-9]+',\"\", text)\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentiment_scores = [analyze_sentiment(sentence) for sentence in sentences if sentence not in stop_words]\n",
    "\n",
    "    total_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences for sentence in sentences)\n",
    "\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    Average_Number_of_words_per_sentence = total_words / total_sentences\n",
    "    Average_Word_Length.append(calculate_avg_word_length(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93119760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new list with the rounded elements from the original Average_Word_Length list:\n",
    "Average_Word_len = []\n",
    "for num in Average_Word_Length:\n",
    "    Average_Word_len.append(round(num, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d505bbaf",
   "metadata": {},
   "source": [
    "# Creating a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9ca02318",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"C:\\\\Users\\\\User\\\\Downloads\\\\Input.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a320e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "index  = data[\"URL_ID\"] # appending the Url_id directly from the excel sheet's first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "05046e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"URL ID\":index,'URL': URL, 'Positive Score': Positive_Score, 'Negative score': Negative_score, \n",
    "                   'Polarity Score': Polarity_Score, 'Subjectivity Score': Subjectivity_Score,\n",
    "                  \"Average sentence length\":Average_sentence_length, \"Percentage of complex words\":Percentage_of_complex_words,\n",
    "                  \"Fog Index\":Fog_Index,\n",
    "                   \"Average Number of words per sentence\":Average_Number_of_words_per_sentence,\n",
    "                   \"Complex words count\":Complex_words_count,\n",
    "                   \"Word count\":Word_count,\n",
    "                  \"Personal Pronouns\":Personal_pronoun_counts,\n",
    "                  \"Average Word length\":Average_Word_len})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "81147785",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a81833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e018f304",
   "metadata": {},
   "source": [
    "# Extracting as Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "823c17a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting to Excel\n",
    "#df.to_excel('Output Data.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
